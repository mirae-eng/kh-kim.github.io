---
layout: page
title: 딥러닝에서의 한국어 segmentation (Korean segmentation for deep learning task)
---

요근래 Chatbot을 만드는 일을 시작 하였다. 따라서 Sentence Classification, Text Summarization, Dialogue Modeling, QnA system 같은 sub-task가 생겼는데, 많은 부분을 딥러닝을 통해서 해결하려다보니, 기존의 NLP와는 다른 점들이 눈에 띄여 글을 남겨 본다. 이 글은 딥러닝에서 한국어의 segmentation에 대해서 다루어 보고자 한다.

#### 먼저 사전 지식을 조금 정리하고 넘어가도록 하자면, ####
한국어는 교착어에 속하는 언어이다. 따라서 굴절어에 속하는 영어권 언어와 달리 띄어쓰기가 있지만, 단어에 접사(affix)가 붙어있기 때문에 기존의 띄어쓰기 그대로 활용 할 경우에 단어의 형태와 역할이 달라지게 된다. (따라서 단어의 위치는 그다지 중요하지 않다.) 예를 들어 "차+가", "차+를", "차+에", "차+에서", "차+와" 등 "차"라는 단어에 대해서 모두 다른 형태를 띄는 것을 알 수 있었다. 사람은 무의식적으로 "차"라는 단어에 집중을 두기 때문에 결국 비슷한 단어로 치부해버리고 넘어가버리지만, 컴퓨터의 경우에는 모두 각기 다른 문자열일 뿐이다.

따라서, 예를 들어 language modeling을 하는 작업에 있어서 raw text 그대로 활용하게 된다면, "차"라는 단어에 대해서 엄청나게 많은 부가적인 단어들이 딸려오게 되므로, 사전의 크기는 엄청나게 (쓸모 없게) 증가 할 것이고, "따뜻한 차가", "따뜻한 차와", "따뜻한 차에", "따뜻한 차에서" 등의 비슷한 의미의 phrase에 대해서 모두 다른 n-gram (또는 확률)이 생성되어 sparsity가 증가할 것이다. 결국 sparsity가 증가한 만큼 더 많은 데이터(corpus)가 필요할 것이다. (사실, 교착어는 어순이 중요하지 않기 때문에 안그래도 Language Modeling 할 때에 굴절어에 비해서 훨씬 더 불리하다. - 물론 중국어와 같은 고립어 보단 낫다...)

더군다나 영어와 달리 띄어쓰기도 (표준 띄어쓰기가 존재하지만) 제각각이기 마련이어서, (실제 이 article만 해도 표준 띄어쓰기와는 차이가 있을 것이다.) raw text의 띄어쓰기만을 믿고 단어들을 counting 하면 잘못된 결과를 얻기 마련이다. - 사실 딥러닝을 해도 이 문제는 남아 있다.

따라서 전통적인 방식의 NLP를 수행할 때에는, 위의 예제와 같이 마치 영어의 lemmatization 처럼, "차가" → "차", "+가" 로 segmenting 해주어야 한다. 이를 통해 접사로 인해 생기는 sparsity를 조금이나마 해소할 수 있었다. 예를 들어, 위와 같은 형태의 segmentation을 통해서 "따뜻 +한 차 +가", "따뜻 +한 차 +와", "따뜻 +한 차 +에" 등와 같이 통계적으로 좀 더 유리함을 얻을 수 있다.

#### 서문이 길었다. ####
이제 딥러닝에서의 NLP문제는 기존과 다르게 접근하므로 기존의 segmentation 방식은 좋지 않을 수 있다. 대부분의 딥러닝 NLP 문제는 단어 자체를 바로 사용하기 보단, Word Vector Embedding을 사용하여 high dimensional space에 projection 된 형태의 vector로써 단어를 나타낸다. 이때의 vector들은 semantic 또는 syntactic 한 유사성에 따라서 비슷한 단어끼리 비슷한 방향성을 지닌다. (가장 많이 사용되는 Embedding 방법으로는 널리 알려진 Word2vec이 있는데, 이 글도 Word2vec에 초점을 맞추어 풀어가겠다.)

- "나는 따뜻한 차를 마신다."
- "너와 나는 따뜻한 커피를 마신다."
- "나는 따뜻한 커피를 마시는 것을 좋아한다."
- "나는 따뜻한 차를 원한다."

위와 같은 문장들이 주어졌을 때에, 앞뒤로 사용 된 단어가 많이 겹치게 되므로, word2vec은 "차"와 "커피"를 비슷한 방향을 지닌 vector로 훈련한다. 

하지만 이 뿐만이 아니다. 문제는 기존의 segmentation 방식을 사용하게 되면 "차"와 앞뒤로 사용 된 단어가 많이 겹치는 단어가 이외에도 많다는 것이다 - "따뜻", "+한", "+를". (물론 "따뜻"이란 단어는 "차", "커피"등과 연관 되어지는 것이 자연스럽다.) 이는 우리가 원한 결과가 아닐 지도 모른다. 기존의 NLP문제에서 sparsity문제를 해결하기 위해서 잘게 쪼개어진 단어(문장)들이 이제는 문제가 되어버렸다.

기존의 방식과 달리 이제 컴퓨터는 단어간의 거리를 계산할 수 있기 때문에 - 즉, "마신다", "마십니다", "마시다", "마셨다" 등을 비슷한 vector로 치환할 수 있다, (시제(과거/현재/미래)나 다른 문법적인 요소를 고려하지 않고) 의미적으로 파악할 때에는 굳이 "마시 +ㄴ +다", "마시 +ㅂ니다", "마시 +다" 등으로 전처리 segmentation을 하지 않아도 되는 것이다. (다만, "+을", "+를", "+에"와 같이 주어와 목적어 등을 결정하는 접사는 다를 수 있다는 생각이 든다.)

접사라던가 합성명사 등을 더 쪼개어 버리게 되면, 특히 Convolution Filter의 경우에는 같은 의미의 phrase에 대해서 다른 갯수의 filter가 사용되는 일이 필요해지게 되므로 불리해 지게 된다.

그러므로 이제는 한글 segmentation 적용에 있어서 좀 더 느슨하게 해도 될지도 모른다. 어쩌면 표준 띄어쓰기 정도만 맞춰주는 형태로도 충분할 수도 있다는 생각이 들었다. **실제로도 KoNlPy에 포함되어 있는 Mecab(은전한닢), Komoran, Twitter 형태소 분석기를 가지고 일대일 문의 게시판의 글을 segmenting 한 이후에 Topic Classification을 할 경우에 좀더 rough 한 형태의 segmenting이 제공되는 Twitter가 더 높은 성능을 나타냄을 볼 수 있었다.**

이를 좀더 확실하게 검증하기 위해서는 이미 표준 띄어쓰기가 적용 되어 있는 text를 segmentation 한 이후에 Sentence Classification 등을 비교/진행 해 보면 될 듯 하다. 대표적인 표준 띄어쓰기 되어 있는 text로는 뉴스기사를 택할 수 있을 것이다. 따라서, 뉴스 headline을 뉴스 주제 별로 분류 하는 classifier 또는 summarization 하는 deep learning network를 만들어 보면 될 듯 하다.

---

#### 추가 ####
Word2Vec 대신에 GloVe를 같은 Corpus에 대해서 돌려 보았더니 접사와 같은 frequent한 단어에 대해서는 위의 문제가 어느정도 해결 됨을 알 수 있었다. GloVe를 구할 때에 단어의 frequency에 대해서 disadvantage를 주기 때문으로 해석 된다. 
