---
layout: page
title: Recommender System and Natural Language Processing
---
현재 나는 티켓몬스터에서 추천 시스템을 개발 하고 있지만, 과거에는 ETRI에서 음성인식을 위한 Language Modeling과 NLP를 담당하였다. 
따라서 Data Mining과 연관된 업무를 하고 있지만, 자연스레 과거의 NLP쪽 업무 경력이 현재 업무에 영향을 미칠 수 밖에 없다.
내가 보기에는 추천 시스템과 NLP는 굉장히 비슷한 문제이고, 유사한 방법으로 해결 할 수 있다. 이와 관련해서 이번 포스팅을 적어 보고자 한다.

추천 시스템의 본질은 사용자(고객)의 이전까지의 클릭/구매 이력을 바탕으로 다음 클릭/구매 이력을 예측 하는 것이다. 따라서 아래와 같은 수식으로 표현 될 수 있다.

<b><i><center>P(Product_i | Product_1, Product_2, ... , Product_i-1)</center></i></b>

위의 수식은 Language Modeling과 매우 유사한 수식이 된다. Language Modeling도 기본적으로 이전까지 출현 한 단어가 주어지면 다음 단어를 예측 하는 것이기 때문이다.
따라서, 여기에서 우리는 상품과 단어를 치환하여 문제를 적용시켜 볼 수 있음을 알 수 있다.

또한, 같은 시각에서, 사용자를 클릭/구매 한 상품들의 sequence로 정의 할 수 있다고 할 때에, 상품들이 단어와 같은 존재이므로, 사용자를 문장으로 접근하여 생각 해 볼 수 있다.

<b><i><center>Product ↔ Word</center></i></b>

<b><i><center>↓</center></i></b>

<b><i><center>User ↔ Sentence</center></i></b>

마찬가지로, 위에서 볼 수 있듯이, 추천 시스템(Recommender System)과 자연어처리(NLP)는 굉장히 비슷한 문제를 풀고 있음을 알 수 있고, 실제로 이 둘은 모두 Sparsity와 힘겨운 싸움을 해 왔다.
단어도 수십만 단어가 존재하지만, 상품도 수십만 가지가 존재하고, 사용자는 수백~수천만명이 존재한다.

Sparsity를 해결하기 위하여 Language Modeling에서는 back-off 방식의 n-gram을 사용해 왔고, 
추천 시스템에서는 SVD와 같은 dimension reduction 또는 Collaborative Filtering (CF) 방식을 통해 효과적인 추천을 하였다. 
그리고 LM과 달리 기존의 추천 방식은 꽤 훌륭하게 동작한다. 어쩌면 NLP에 비하면 훨씬 더 쉬운 문제일 수도 있다는 생각이 들기도 한다. (덕분에 baseline이 높아졌지만)

최근 수년간 딥러닝(Deep Learning)이 여러 Machine Learning 영역을 정복 해 왔고, NLP도 예외가 아니었다.
Word2Vec이 제안 된 이래로, 단어를 효과적으로 vector로 표현할 수 있게 되면서, 수많은 가시적인 성과들이 쏟아졌다.
그렇다면, 비슷한 문제를 풀어야 하는 추천 시스템에도 유사한 방법으로 적용이 가능 할 것이다.

현재 여러가지 NLP 알고리즘들을 추천 시스템에 적용 하는 것을 시도 해 보고 있지만, 먼저 검증 된 알고리즘(?)을 article에 적어본다.
추후 딥러닝을 활용한 최신의 새로운 NLP 기법들을 적용하는 방식은 다른 글을 통해 소개하도록 하겠다.

가장 먼저 해야 할 일은, 이후 NLP 문제들을 풀기 위한 선결 과제로 상품을 vector화 하는 것이다.
위에서 언급했듯이, 상품은 단어로써 치환 될 수 있다. 
이를 그대로 word2vec에 적용 시켜 보는 것은 어떨까?

사용자의 클릭 history를 한 문장으로 나타내어 시간 순으로 나열하고, word2vec을 통해 훈련 시킨 후 결과를 보면, 앞/뒤 클릭 내용이 비슷한 상품끼리 비슷한 vector를 갖도록 훈련 되는 것을 확인할 수 있었다.
이는 상품의 대체제 추천에 사용 될 수 있다.
또한 더 나아가 동시에 구매한 상품들을 한 문장으로 나타내어 훈련시킨다면, 함께 구매한 물건들이 비슷한 상품끼리 비슷한 vector 값을 갖도록 훈련하게 되어, 보완재 추천의 기능을 할 수 있게 된다.

word2vec을 적용 시킬 때에 몇가지 튜닝이 필요한 parameter들이 있는데, 가장 중요한 것 중에 하나는 window size이다.
보통 Skip-gram 알고리즘을 가장 많이 사용하게 되는데, 알고리즘 내부를 살펴보면, window size 내에서는 순서가 상관이 없다.
따라서, 실제로 더 큰 window size를 사용할 수록, vector는 syntax 보다는 semantic 정보를 포함하게 된다.
이를 활용하면 원하는 방식으로 추천 시스템을 구성 할 수 있다. 
예를 들어 동시에 구매한 상품들을 한 문장으로 나타내어 훈련 시키는 보완재 추천의 경우에는, window size를 매우 크게 잡음으로써, 사실상 함께 구매 한 것이므로 문장 내에서의 순서가 의미가 없는 부분을 상쇄하여 보완하였다.

이처럼 추천 시스템은 NLP문제로 치환하여 비슷한 방식으로 접근하여 문제를 해결 할 수 있을 뿐만 아니라, sparsity의 어려움을 해결함으로써, 기타 다른 해결방법을 적용하는 것도 가능해졌다.
그 중에 가장 해볼만한 것은 Deep Reinforcement Learning 이라고 할 수 있겠다.

다음 포스팅에서는 다른 딥러닝 NLP 알고리즘을 활용하여 추천시스템을 구현하는 것에 대해서 다루도록 하겠다.